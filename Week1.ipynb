{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Phase 1 - Ingestion and Cleaning\n",
    "\n",
    " This notebook carries out the following steps\n",
    "   1. Ingest data downloaded from the LendingClub website\n",
    "   2. Choose columns to examine and prepare the data set\n",
    "   3. Visualizes single variable summaries\n",
    "   4. Removes outliers\n",
    "   5. Output a dataset ready for later analysis\n",
    "\n",
    " Things for you to add\n",
    " - Choose 3 to 5 variables and add them to the list of variables below\n",
    " - Try visualizing the new variables and other pairs of variables\n",
    "\n",
    " Prepare your presentation. Your presentation should contain at most 6 slides.\n",
    " 1. Begin by giving an overview of the project. What is the problem you wish to solve, what are the objectives? How will you evaluate the performance of the portfolio you provide? How will you measure success? What are the business KPIs?\n",
    " 2. What variables did you select for further inspection? Why do you think they will be useful? You may support any argument with a visualization.\n",
    " 3. List any insight you gained by looking at the data visualization or any other data analysis that you perform.\n",
    " 4. List 3-5 hypothesis about which variables will be important for analysis, and how they will affect the outcome.\n",
    " 5. State your conclusions. What is the main idea you wish to convey with the presentation? Do you think the data available will be useful to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sys import platform\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_integer(x):\n",
    "    '''\n",
    "    This function returns True if x is an integer, and False otherwise\n",
    "    '''\n",
    "    try:\n",
    "        return (int(x) == float(x))\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can download the archived data for 2014 from the WayBack machine here: \n",
    "# https://web.archive.org/web/20160703081246/https://resources.lendingclub.com/LoanStats3c.csv.zip\n",
    "# For the 2012-13 data, use https://web.archive.org/web/20160703081246/https://resources.lendingclub.com/LoanStats3b.csv.zip \n",
    "# Download both zip files and unzip them.\n",
    "# Put both these unzipped files in the appropriate folder (\"../data\" below) before proceeding below.\n",
    "# The data dictionary is here: https://web.archive.org/web/20200606105339/https://resources.lendingclub.com/LCDataDictionary.xlsx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = \"C:\\\\Users\\\\rodol\\\\OneDrive\\\\Documents\\\\Training\\\\Carnegie_Mellon_University\\\\Courses\\\\Mini_7\\\\Business_Value_Analytics\\\\General\\\\WK1\\\\Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 1 - Ingestion\n",
    " Ingest the data files from both sets, perform consistency checks, and prepare one single file for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, final\n",
    "def ingest_files(directory: str) -> Dict:\n",
    "    '''\n",
    "    This function will ingest every file in the specified directory\n",
    "    into a pandas dataframe. It will return a dictionary containing\n",
    "    these dataframes, keyed by the file name.\n",
    "    \n",
    "    We assume the directory contains files directly downloaded from\n",
    "    Lending Club, and *only* those files. Thus, we assume the files are zipped\n",
    "    (pd.read_csv can read zipped files) and we assume the first line\n",
    "    in each file needs to be skipped. \n",
    "    \n",
    "    Note that this function will read and ingest more than one file and is\n",
    "    convenient if you want to ingest data for more than one year at a time.\n",
    "    \n",
    "    Note that each file will be read *without* formatting\n",
    "    '''\n",
    "    \n",
    "    # If the directory has no trailing slash, add one\n",
    "    if directory[-1] != \"/\":\n",
    "        directory = directory + \"/\"\n",
    "    \n",
    "    all_files = os.listdir(directory)\n",
    "    output = {}\n",
    "    \n",
    "    print(\"Directory \" + directory + \" has \" + str(len(all_files)) + \" files:\")\n",
    "    for i in all_files:\n",
    "        print(\"    Reading file \" + i)\n",
    "        output[i] = pd.read_csv(directory + i, dtype = str, skiprows = 1)\n",
    "        \n",
    "        # Some of the files have \"summary\" lines that, for example\n",
    "        # read \"Total number of loans number in Policy 1: .....\"\n",
    "        # To remove those lines, find any lines with non-integer IDs\n",
    "        # and remove them\n",
    "        invalid_rows = (output[i].id.apply( lambda x : is_integer(x) == False ))\n",
    "        if invalid_rows.sum() > 0:\n",
    "            print(\"        Found \" + str(invalid_rows.sum()) + \" invalid rows which were removed\")\n",
    "            output[i] = output[i][invalid_rows == False]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the set of files we downloaded \n",
    "files_data = ingest_files(dir_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_now = pd.concat(files_data.values()).reset_index(drop = True)\n",
    "columns = list(data_now.columns)\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_now.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 2 - Choose Columns and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns we'll be keeping from the dataset\n",
    "cols_to_pick = ['id','loan_amnt','funded_amnt','term','int_rate',\n",
    "                 'installment','grade','emp_length', 'home_ownership',\n",
    "                 'annual_inc','verification_status','issue_d',\n",
    "                 'loan_status','purpose','dti', 'delinq_2yrs',\n",
    "                 'earliest_cr_line','open_acc','pub_rec',\n",
    "                 'revol_bal','revol_util', 'total_pymnt',\n",
    "                 'last_pymnt_d', 'recoveries']\n",
    "\n",
    "# Identify the type of each of these column\n",
    "float_cols = ['loan_amnt', 'funded_amnt', 'installment', 'annual_inc',\n",
    "                     'dti', 'revol_bal', 'delinq_2yrs', 'open_acc', 'pub_rec',\n",
    "                                'total_pymnt', 'recoveries']\n",
    "cat_cols = ['term', 'grade', 'emp_length', 'home_ownership',\n",
    "                    'verification_status', 'loan_status', 'purpose']\n",
    "perc_cols = ['int_rate', 'revol_util']\n",
    "date_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d']\n",
    "\n",
    "# Ensure that we have types for every column\n",
    "assert set(cols_to_pick) - set(float_cols) - set(cat_cols) - set(perc_cols) - set(date_cols) == set([\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns of interest\n",
    "final_data = data_now[cols_to_pick].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting with \" + str(len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # To do (A)\n",
    "\n",
    " Choose 3 to 5 variables and add them to the list of variables below\n",
    "\n",
    " You should consult the data description (excel) file you downloaded to understand the definition of various available columns\n",
    "\n",
    " TIP: If you added new variables, be sure to clean them as we just did for the default variables.\n",
    "\n",
    " You will have to add them to the group of the right type of variables (e.g. percentage, date, categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the columns to pick\n",
    "cols_to_pick.extend(['zip_code', 'sub_grade', 'inq_last_6mths', 'acc_now_delinq', 'mths_since_last_delinq'])\n",
    "\n",
    "# Update the type of each of these new columns\n",
    "float_cols.extend(['inq_last_6mths', 'acc_now_delinq', 'mths_since_last_delinq'])\n",
    "cat_cols.extend(['zip_code', 'sub_grade'])\n",
    "\n",
    "# Ensure that we have types for every column\n",
    "assert set(cols_to_pick) - set(float_cols) - set(cat_cols) - set(perc_cols) - set(date_cols) == set([\"id\"])\n",
    "# Keep only the columns of interest\n",
    "final_data = data_now[cols_to_pick].copy()\n",
    "\n",
    "print(\"Starting with \" + str(len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Typecast the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in float_cols:\n",
    "    final_data[i] = final_data[i].astype(float)\n",
    "    \n",
    "def clean_perc(x):\n",
    "    if pd.isnull(x):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return float(x.rstrip()[:-1])\n",
    "    \n",
    "for i in perc_cols:\n",
    "    final_data[i] = final_data[i].apply( clean_perc )\n",
    "    \n",
    "def clean_date(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    else:\n",
    "        return datetime.datetime.strptime( x, \"%b-%Y\").date()\n",
    "for i in date_cols:\n",
    "    final_data[i] = final_data[i].apply( clean_date )\n",
    "    \n",
    "for i in cat_cols:\n",
    "    final_data.loc[final_data[i].isnull(), i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Handling missing values\n",
    "# final_data['emp_length'].fillna('Not Provided', inplace=True)  # Filling missing values with 'Not Provided'\n",
    "# final_data['revol_util'].fillna(final_data['revol_util'].median(), inplace=True)  # Filling with median\n",
    "# final_data['last_pymnt_d'].fillna('No Payment', inplace=True)  # Filling missing values with 'No Payment'\n",
    "\n",
    "# # For 'mths_since_last_delinq', it's a large number of missing values.\n",
    "# # Depending on business logic, you might want to fill it with a value or create a flag column\n",
    "# final_data['mths_since_last_delinq'].fillna(999, inplace=True)  # Filling with 999 to signify missing data\n",
    "\n",
    "# # Check if all missing values are handled\n",
    "# assert final_data.isnull().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 3- Visualize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4,4))\n",
    "def visualize_columns():\n",
    "    \n",
    "    '''\n",
    "    This function visualizes all columns\n",
    "      - Box-and-whisker plots for continuous variables\n",
    "      - Lists of distinct values for categorical columns\n",
    "      - A timeline density for dates\n",
    "    '''\n",
    "    \n",
    "    # Float columns\n",
    "    for i in float_cols + perc_cols:\n",
    "        # seaborn.boxplot(final_data[i])\n",
    "        final_data.boxplot(i)\n",
    "\n",
    "        # Print the three highest values\n",
    "        highest_vals = sorted(final_data[i], reverse=True)[:3]\n",
    "        smallest_val = min(final_data[i])\n",
    "        print(\"Top 3 Max: \", highest_vals, \"\\nMin: \", smallest_val)\n",
    "        # plt.text(smallest_val, -0.3, highest_vals[0])\n",
    "        # plt.text(smallest_val, -0.2, highest_vals[1])\n",
    "        # plt.text(smallest_val, -0.1, highest_vals[2])\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    # Categorical columns \n",
    "    for i in cat_cols:\n",
    "        print(i)\n",
    "        print(str(len(set(final_data[i]))) + \" distinct values\")\n",
    "        print(final_data[i].value_counts())\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Date columns\n",
    "    for i in date_cols:\n",
    "        final_data[final_data[i].isnull() == False][i].apply(lambda x : str(x.year) +\n",
    "                                                \"-\" + str(x.month)).value_counts(ascending = True).plot()\n",
    "        plt.title(i + \" (\" + str(final_data[i].isnull().sum()) + \" null values)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# def visualize_columns(df, float_cols, perc_cols, cat_cols, date_cols):\n",
    "#     '''\n",
    "#     This function visualizes all columns\n",
    "#       - Box-and-whisker plots for continuous variables\n",
    "#       - Lists of distinct values for categorical columns\n",
    "#       - A timeline density for dates\n",
    "#     '''\n",
    "    \n",
    "#     # Float columns\n",
    "#     for i in float_cols + perc_cols:\n",
    "#         plt.figure(figsize=(8, 4))\n",
    "#         sns.boxplot(x=df[i])\n",
    "#         plt.grid(True)\n",
    "        \n",
    "#         # Print the three highest values\n",
    "#         highest_vals = sorted(df[i].dropna(), reverse=True)[:3]\n",
    "#         smallest_val = min(df[i].dropna())\n",
    "#         plt.title(f\"{i}\\nTop 3 Max: {highest_vals} \\nMin: {smallest_val}\")\n",
    "#         plt.show()\n",
    "        \n",
    "#     # Categorical columns \n",
    "#     for i in cat_cols:\n",
    "#         plt.figure(figsize=(8, 4))\n",
    "#         sns.countplot(data=df, x=i, order=df[i].value_counts().index)\n",
    "#         plt.title(f\"{i} - {len(df[i].unique())} distinct values\")\n",
    "#         plt.xticks(rotation=90)\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "    \n",
    "#     # Date columns\n",
    "#     for i in date_cols:\n",
    "#         plt.figure(figsize=(8, 4))\n",
    "#         df[df[i].isnull() == False][i].apply(lambda x : str(x.year) +\n",
    "#                                             \"-\" + str(x.month)).value_counts(ascending = True).sort_index().plot(kind='bar')\n",
    "#         plt.title(f\"{i} ({df[i].isnull().sum()} null values)\")\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "\n",
    "# # Usage example:\n",
    "# # visualize_columns(final_data, float_cols, perc_cols, cat_cols, date_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # To do (B)\n",
    "\n",
    " Try visualizing the new variables and other pairs of variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def visualize_new_columns_with_plotly(final_data, float_cols, cat_cols):\n",
    "    \n",
    "    # Float columns\n",
    "    for i in float_cols:\n",
    "        fig = px.box(final_data, y=i, title=f\"Boxplot of {i}\")\n",
    "        fig.show()\n",
    "        \n",
    "        # Print the three highest values and the smallest value\n",
    "        highest_vals = sorted(final_data[i].dropna(), reverse=True)[:3]\n",
    "        smallest_val = final_data[i].min()\n",
    "        print(f\"Top 3 Max for {i}: {highest_vals} \\nMin for {i}: {smallest_val}\\n\")\n",
    "        \n",
    "    # Categorical columns\n",
    "    for i in cat_cols:\n",
    "        if i == 'zip_code':\n",
    "            # Group by the first 3 digits of the zip code to represent broader geographical regions\n",
    "            final_data['zip_region'] = final_data['zip_code'].str[:3]\n",
    "            fig = px.histogram(final_data, x='zip_region', title=f\"Countplot of {i} by region\")\n",
    "        else:\n",
    "            fig = px.histogram(final_data, x=i, title=f\"Countplot of {i}\")\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        print(f\"{i} has {len(final_data[i].unique())} distinct values.\")\n",
    "        print(final_data[i].value_counts(), '\\n')\n",
    "\n",
    "# Newly added float columns\n",
    "new_float_cols = ['inq_last_6mths', 'acc_now_delinq', 'mths_since_last_delinq']\n",
    "\n",
    "# Newly added categorical columns\n",
    "new_cat_cols = ['zip_code', 'sub_grade']\n",
    "\n",
    "# Assuming final_data contains your actual data\n",
    "# final_data = data_now[cols_to_pick].copy()\n",
    "\n",
    "# Call the visualize_new_columns_with_plotly function\n",
    "visualize_new_columns_with_plotly(final_data, new_float_cols, new_cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Your existing lists of columns and their types\n",
    "# cols_to_pick = ['id','loan_amnt','funded_amnt','term','int_rate',\n",
    "#                 'installment','grade','emp_length','home_ownership',\n",
    "#                 'annual_inc','verification_status','issue_d',\n",
    "#                 'loan_status','purpose','dti','delinq_2yrs',\n",
    "#                 'earliest_cr_line','open_acc','pub_rec',\n",
    "#                 'revol_bal','revol_util','total_pymnt',\n",
    "#                 'last_pymnt_d','recoveries', 'zip_code', 'sub_grade', \n",
    "#                 'inq_last_6mths', 'acc_now_delinq', 'mths_since_last_delinq']\n",
    "\n",
    "# float_cols = ['loan_amnt', 'funded_amnt', 'installment', 'annual_inc',\n",
    "#               'dti', 'revol_bal', 'delinq_2yrs', 'open_acc', 'pub_rec',\n",
    "#               'total_pymnt', 'recoveries', 'inq_last_6mths', 'acc_now_delinq', \n",
    "#               'mths_since_last_delinq']\n",
    "\n",
    "# cat_cols = ['term', 'grade', 'emp_length', 'home_ownership',\n",
    "#             'verification_status', 'loan_status', 'purpose', 'zip_code', 'sub_grade']\n",
    "\n",
    "# perc_cols = ['int_rate', 'revol_util']\n",
    "\n",
    "# date_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d']\n",
    "\n",
    "# # Ensure that we have types for every column\n",
    "# assert set(cols_to_pick) - set(float_cols) - set(cat_cols) - set(perc_cols) - set(date_cols) == set([\"id\"])\n",
    "\n",
    "# # Assuming final_data contains your actual data\n",
    "# # final_data = data_now[cols_to_pick].copy()\n",
    "\n",
    "# # Call the visualize_columns function\n",
    "# visualize_columns(final_data, float_cols, perc_cols, cat_cols, date_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 4 - Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are quite a few outliers, but the two most obvious\n",
    "# ones to remove are in annual_inc, revol_bal Remove these.\n",
    "n_rows = len(final_data)\n",
    "final_data = final_data[final_data.annual_inc < 1000000]\n",
    "final_data = final_data[final_data.revol_bal < 400000]\n",
    "final_data = final_data[final_data.dti < 200]\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all loans that are too recent to have been paid off or\n",
    "# defaulted\n",
    "n_rows = len(final_data)\n",
    "final_data = final_data[final_data.loan_status.isin(['Fully Paid','Charged Off','Default'])]\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include loans issued since 2009\n",
    "n_rows = len(final_data)\n",
    "final_data = final_data[final_data.issue_d >= datetime.date(2009, 1, 1)]\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data again\n",
    "visualize_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with null values. We allow categorical variables to be null\n",
    "# OTHER than grade, which is a particularly important categorical.\n",
    "# All non-categorical variables must be non-null, and we drop\n",
    "# rows that do not meet this requirement\n",
    "required_cols = set(cols_to_pick) - set(cat_cols) - set([\"id\"])\n",
    "required_cols.add(\"grade\")\n",
    "\n",
    "n_rows = len(final_data)\n",
    "final_data.dropna(subset = required_cols ,inplace=True)\n",
    "print(\"Removed \" + str(n_rows - len(final_data)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the columns selected will not be used directly in the model, but will be used to generate other features.\n",
    "# Create variables specifying the features that will be used\n",
    "\n",
    "# All categorical columns other than \"loan_status\" will be used as discrete features\n",
    "discrete_features = list(set(cat_cols) - set([\"loan_status\"]))\n",
    "\n",
    "# All numeric columns will be used as continuous features\n",
    "continuous_features = list(float_cols + perc_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 5 - Save a Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the total_pymnt from the list of continuous features; this\n",
    "# variable is highly predictive of the outcome but is not known at\n",
    "# the time the loan is issued\n",
    "continuous_features = [i for i in continuous_features if i not in [\"total_pymnt\", \"recoveries\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path for the pickle\n",
    "pickle_file = \"/\".join(['.', \"PickleData\", \"clean_data.pickle\"])\n",
    "os.makedirs(os.path.dirname(pickle_file), exist_ok=True)\n",
    "pickle.dump( [final_data, discrete_features, continuous_features], open(pickle_file, \"wb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Read from Pickle if Saved\n",
    " Read data from saved pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the data and features from the pickle\n",
    "final_data, discrete_features, continuous_features = pickle.load( open( \"./PickleData/clean_data.pickle\", \"rb\" ) )\n",
    "#final_data, discrete_features, continuous_features = pickle.load( open( \"./PickleData/201213clean_data.pickle\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 6 Prepare your presentation.\n",
    "\n",
    " Your presentation should contain at most 6 slides.\n",
    "\n",
    " 1) Begin by giving an overview of the project. What is the problem you wish to solve, what are the objectives?\n",
    "\n",
    " How will you evaluate the performance of the portfolio you provide? How will you measure success?\n",
    "\n",
    " What are the business KPIs?\n",
    "\n",
    " 2) What variables did you select for further inspection? Why do you think they will be useful?\n",
    "\n",
    " You may support any argument with a visualization.\n",
    "\n",
    " 3) List any insight you gained by looking at the data visualization or any other data analysis that you perform.\n",
    "\n",
    " 4) List 3-5 hypothesis about which variables will be important for analysis, and how they will affect the outcome.\n",
    "\n",
    " 5) State your conclusions. What is the main idea you wish to convey with the presentation? Do you think the data available will be useful to solve the problem?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
